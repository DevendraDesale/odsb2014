{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Clustering the News with Spark and MLLib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've previously looked at using Spark for both the analysis of text and some machine learning tasks via the PySpark interface.  Through this, we've learned about what words are important over time, and what articles are about.  However, what if we wanted to understand what sort of categories the news breaks into?  This might mean that we'd have to use both our text processing skills and some machine learning.\n",
      "\n",
      "In this lesson, we'll do just that: we'll use a simple unsupervised machine learning method, k-means clustering, to determine what broad categories the news fits into.  To do this, we'll use Spark and it's MLLib libraries via the Scala programming language.  This means the following notebook is **not** interactive.  All of the commands can be copied into Spark's interactive Scala shell (launch it by typing `spark-shell`) or by building a standalone application.\n",
      "\n",
      "We'll discuss building and running a standalone app at the end of the lesson."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll begin with our imports.  We'll need a few things: the MLLib classes that we require and the json4s package for parsing JSON in Scala."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import org.json4s._\n",
      "import org.json4s.jackson.Serialization.{read,write}\n",
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.mllib.clustering.KMeans\n",
      "import org.apache.spark.mllib.feature.Word2Vec\n",
      "import org.apache.spark.mllib.feature.Word2VecModel\n",
      "import org.apache.spark.mllib.linalg._"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we parse our JSON, we're going to want a class to put it in.  Rather than treating it like a python dictionary, we're going to use a Scala Case Class.  This lets us get a full Scala class with just a single line declaration."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "case class NewsArticle(date : String, title : String, byline : String, fulltext : String)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need a number of helper functions for our lesson, so we'll define them here.  Don't worry about what they do yet, we'll cover than in a moment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sumArray (m: Array[Double], n: Array[Double]): Array[Double] = {\n",
      "  for (i <- 0 until m.length) {m(i) += n(i)}\n",
      "  return m\n",
      "}\n",
      "\n",
      "def divArray (m: Array[Double], divisor: Double) : Array[Double] = {\n",
      "  for (i <- 0 until m.length) {m(i) /= divisor}\n",
      "  return m\n",
      "}\n",
      "\n",
      "def wordToVector (w:String, m: Word2VecModel): Vector = {\n",
      "  try {\n",
      "    return m.transform(w)\n",
      "  } catch {\n",
      "    case e: Exception => return Vectors.zeros(100)\n",
      "  }  \n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we're read to get started analyzing data.  Let's load up our news data using, as before, `sc.textFile`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val news_rdd = sc.textFile(\"hdfs://localhost:8020/user/oracle/flu_news\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to parse the JSON data into objects, so, as with our PySpark work, we'll use the `map` function.  However, we're using json4s' mechanisms.  This means we'll use the `read` operation and provide it a *type* of `NewsArticle`.  Unlike Python, Scala is a strongly-typed language.  If the distinction is new to you, try to read up a bit on either Scala basics or on the importance of type to programming languages."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val news_json = news_rdd.map(record => {\n",
      "  implicit val formats = DefaultFormats\n",
      "  read[NewsArticle](record)\n",
      "})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We're planning to use [k-means](http://en.wikipedia.org/wiki/K-means_clustering) clustering to determine automatically which news articles belong to which clusters.  However, we have to deal with a bit of an inconsistency first.  K-means operates on numeric vectors (i.e., points in space), but we have words, not vectors in our articles.  One way to treat this would be to compute TF-IDF for each article and treat that as a point in space.  Each word would be a dimension, and each each TF-IDF score would be the value in that dimension.\n",
      "\n",
      "Ask yourself, how big would that vector be?  Would it only include the dimensions for the words in the article?  For the words in all articles?  Maybe the vectors need to be as big as all the words in the English language!\n",
      "\n",
      "In fact, MLLib has a built-in TF-IDF transform which produces, by default, vectors that are 2^20 long.  That's way too big to deal with in our virtual machine.\n",
      "\n",
      "What would happen if we did the following\n",
      "\n",
      "+ Made each article's vector include only the dimensions of the words in each title?\n",
      "+ Made each article's vector include the dimensions of only the words in all titles?\n",
      "+ Made each article's vector a dimensional reduction of all the words in all titles?\n",
      "\n",
      "For simplicity, we're only going to deal with the titles, as opposed to all the words in the articles.  We're also not going to use TF-IDF, for reasons that will become apparent if you answer the questions above.  Instead, we're going to rely on a method called [Word2Vec](https://code.google.com/p/word2vec/).  Originated at Google, word2vec does a remarkably good job of transforming single words into reasonably-sized vectors.  When generated from a large corpus, these vectors allow us to find synonyms with surprising accuracy.\n",
      "\n",
      "So, the first thing we'll need is a corpus of words.  Let's make one from our titles."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val news_titles = news_json.map(_.title.split(\" \").toSeq)\n",
      "val news_title_words = news_titles.flatMap(x => x).map(x => Seq(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to find better synonyms, we should add more words to our corpus.  Let's do just that by grabbing a sample from the `linewise_text_8` file included in `flu_news/data`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val w2v_input = sc.textFile(\"file:///home/oracle/odsb2014/flu_news/data/linewise_text_8\").sample(false, 0.25,2).map(x => Seq(x))\n",
      "val all_input = w2v_input ++ news_title_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we're ready to build a word2vec model from our corpus.  Constructing this model using Spark is easy!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val word2vec = new Word2Vec()\n",
      "val model = word2vec.fit(all_input)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we've got a model which can compute synonyms, but we have another problem.  Titles have many words and word2vec only operates on one of 2 things: words or vectors.  How can we find the synonyms for a whole title?!\n",
      "\n",
      "One of the interesting features of word2vec is that it displays reasonably good synonym prediction when the fectors for words are added together or subtracted.  That is `v(king) - v(man) ~= v(queen)`.  Thus, we could rationalize that a title is just the average vector of all the words in the title.  Let's give that a try.\n",
      "\n",
      "For this, we'll need to use a couple of our helper functions.  \n",
      "\n",
      "+ Inside our Spark RDD's `map` operation, we're going to call Scala's `map` to apply the word2vec model to each word.  \n",
      "+ That gives us a Sequence of Arrays for each title, which we need to\n",
      "  * Sum up\n",
      "  * Divide by the total number of words in the title\n",
      "+ The summing can be handled by using the `reduceLeft` Scala operator.  Look at the helper function and see if you can determine what is happening.\n",
      "+ The dividing is taken care of by the `divArray` helper function\n",
      "\n",
      "Once this is done, we now have RDDs which contain average vectors for each title.  We're ready to cluster!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val title_vectors = news_titles.map(x => new DenseVector(divArray(x.map(m => wordToVector(m, model).toArray).reduceLeft(sumArray),x.length)).asInstanceOf[Vector])\n",
      "\n",
      "val title_pairs = news_titles.map(x => (x,new DenseVector(divArray(x.map(m => wordToVector(m, model).toArray).reduceLeft(sumArray),x.length)).asInstanceOf[Vector]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As with word2vec, Spark's MLLib make k-means clustering easy.  All we need to do is specify the number of clusters and iterations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "var numClusters = 100\n",
      "val numIterations = 25\n",
      "var clusters = KMeans.train(title_vectors, numClusters, numIterations)\n",
      "var wssse = clusters.computeCost(title_vectors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With our cluster model complete, we can assign article titles to clusters.  We can also create RDDs for each of the cluster centers and produce words for their vectors (i.e., make titles for these purely numerical cluster centers)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val article_membership = title_pairs.mapValues(x => clusters.predict(x))\n",
      "val cluster_centers = sc.parallelize(clusters.clusterCenters.zipWithIndex.map{ e => (e._2,e._1)})\n",
      "val cluster_topics = cluster_centers.mapValues(x => model.findSynonyms(x,5).map(x => x(0)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Taking a look at the cluster membership, we can see not everything is a perfect match. But on the whole more articles make sense in the cluster than do not.  It seems we've done a reasonable job classifying the types of stories in the news."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "var sample_topic = cluster_topics.take(10)(6)\n",
      "println(sample_topic._2.mkString(\",\"))\n",
      "\n",
      "var sample_members = article_membership.filter(x => x._2 == 6).take(100)\n",
      "sample_members.foreach{x => println(x._1.mkString(\",\"))}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building a Stand-Alone Application"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To really get the most out of this, we need to build a stand-alone Spark application.  To do this, we'll need to do a few things."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sbt package"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spark-submit --class com.oracle.newscluster.NewsClustering target/scala-2.10/newsclustering_2.10-0.1.jar"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}