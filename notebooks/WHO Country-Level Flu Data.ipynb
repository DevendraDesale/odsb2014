{
 "metadata": {
  "name": "",
  "signature": "sha256:7bca72e089995043bc31f094a29934ac96b1a43fa92cca8e9caa02736dbb9c7a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Loading Simple Delimited Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are lots of tools to load delimited data in to a database for sharing.  But very often, the delimited data we're given doesn't fit the form of the tables we'd like to make.  In our case, the [WHO](http://www.who.int/influenza/gisrs_laboratory/flunet/en/) makes available country-level data for influenza surveillance: what strain, how many samples, and so on for every week of the year.  The data is free to download, but just a raw dump of delimited strings.\n",
      "\n",
      "In this exercise, we'll use basic python and the cx-Oracle to clean up the data and form it into useful tables.  To begin, let's import cx-Oracle, which we'll use to communicate with our database, and a few standard python libraries."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Munging and Pivoting Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import cx_Oracle\n",
      "import re\n",
      "import string\n",
      "from glob import glob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First thing's first: we need a method that will parse one of the `.psv` files which our WHO data comes in.  Take a minute to look at `WHO+EURO.psv`.  How many potential tables do you see in the data?\n",
      "\n",
      "Unfortunately, lines in the files aren't all the same length.  Some rows have 1 delimiter, some have more than 50.  In our method, we've decided that the split between tables occurs where the rows have 2 \"columns.\"  Can you see why?\n",
      "\n",
      "We're going to break our file up into logical chunks, and each chunk will become a table.  For each of these chunks, we'll need to get the name of the table and the data for the table.  We'll also extract the region of the world this data belongs to from the filename.\n",
      "\n",
      "Finding the title and the dataset inside each chunk is fairly simple, but we'll need to do more to create well-formed data we can share with a database.  For this, we'll define a few functions: one for creating the table name, the other for formatting rows.  Then, for each logical chunk, we'll add it to a list of datasets to be inserted into the database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parseWHOCountryFile(filename):\n",
      "  region = re.sub(\"WHO+\", \"\",filename)\n",
      "  region = re.sub(\".psv\", \"\", region)\n",
      "  region = re.sub(\"\\+\",\"\", region)\n",
      "  region = region.split(\"/\")[-1]\n",
      "  raw = open(filename).readlines()\n",
      "  big_splits = []\n",
      "  for i in range(len(raw)):\n",
      "    if len(raw[i].split(\"|\")) == 2:\n",
      "      big_splits.append(i)\n",
      "  datasets = []\n",
      "  for i in range(0,len(big_splits),2):\n",
      "    ds = {}\n",
      "    ds['title'] = raw[big_splits[i]]\n",
      "    ds['title'] = make_tablename(ds['title'])\n",
      "    ds['period'] = raw[big_splits[i+1]]\n",
      "    if i < len(big_splits)-2:\n",
      "      ds['starts'] = big_splits[i+1]+1\n",
      "      ds['ends'] = big_splits[i+2]-1\n",
      "    else:\n",
      "      ds['starts'] = big_splits[i+1]+1\n",
      "      ds['ends'] = len(raw)-1\n",
      "    ds['data'] = raw[ds['starts']:ds['ends']]\n",
      "    ds['data'] = makerows(region, ds['data'])\n",
      "    datasets.append(ds)\n",
      "  return datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Making the name for the table is pretty easy.  We know we can't have special characters like `(` or `)` in our table name.  We also can't have spaces.  So, with a little chopping up of the string and a few calls to the re module, we've got a clean table name."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_tablename(d):\n",
      "  title_string = d.split(\"|\")[0]\n",
      "  if \"specimen\" in title_string:\n",
      "    title = title_string.split(\"->\")[-1].strip()+\"_Specimens\"\n",
      "    if \"processed\" in title_string:\n",
      "      title += \"_proc\"\n",
      "    else:\n",
      "      title += \"_recv\"\n",
      "  else:\n",
      "    title = title_string.split(\"->\")[-1].strip()+\"_Infections\"\n",
      "  title = re.sub(\"\\(\", \"\", title)\n",
      "  title = re.sub(\"\\)\", \"\", title)\n",
      "  title = re.sub(\" \",\"_\", title)\n",
      "  return title[:25]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Separating the lines of data into rows and columns for our database insert is easy: everything is delimited by the `|` character.  However, we'd like to have a table of tuples like this (region, country, year, week, measurement) and instead we've got all the measures for every week of the year on a single line.  Fortunately, this sort of *en masse* string manipulation is easy with python's map function.  By defining an inner function and using the map operator, we can quickly pivot all the weeks into the 4-tuple for our database table."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def makerows(region_code, data):\n",
      "  dateprefix = data[0]\n",
      "  dates = map(lambda x: x.split(), dateprefix.strip().split(\"|\")[1:])\n",
      "  dates = map(lambda x: [x[0], x[-1]], dates)\n",
      "  data = map(lambda x: x.strip().split(\"|\"), data[1:])\n",
      "  # what we're really doing here is pivoting the data so that we can have country, year, week, value\n",
      "  # for each row of raw data, we want a list of tuple (country, year, week, value)\n",
      "  def row_to_tuple(dates, r):\n",
      "    t = []\n",
      "    for i in range(1,len(r)):\n",
      "      t.append((region_code, r[0], int(dates[i-1][0]), int(dates[i-1][1]), int(re.sub(\",\",\"\",r[i]))))\n",
      "    return t\n",
      "  data = map(lambda x: row_to_tuple(dates, x), data)\n",
      "  return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Writing Tables and Views"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have a list of datasets, each with a table name and a set of 4-tuples.  In order to write this to the database, we'll need a method which does bulk inserts into Oracle database.  Our *write_to_db* function does just that.  Notice that because we don't know the name of the table, we use string substitution to automatically create a table for each of the logical chunks in a `psv` file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_to_db(db, dataset):\n",
      "  dt = \"DROP TABLE %s\" % dataset['title']\n",
      "  cursor = db.cursor()\n",
      "  ctine = \"\"\"CREATE TABLE %s (\n",
      "\t  region VARCHAR(12),\n",
      "\t  country VARCHAR(500),\n",
      "\t  year NUMBER,\n",
      "\t  week NUMBER,\n",
      "\t  measure NUMBER,\n",
      "          CONSTRAINT %s_id PRIMARY KEY (country, year, week)\n",
      "          )\"\"\" % (dataset['title'], dataset['title'])\n",
      "  try:\n",
      "    cursor.execute(ctine)\n",
      "  except Exception as e:\n",
      "    print e\n",
      "    print \"failed to create table\", dataset['title']\n",
      "    pass\n",
      "  for country in dataset['data']:\n",
      "    try:\n",
      "      cursor.prepare(\"INSERT INTO %s (region, country, year, week, measure) VALUES (:1, :2, :3, :4, :5)\" % dataset['title'])\n",
      "      cursor.executemany(None, country)\n",
      "      db.commit()\n",
      "    except Exception as e:\n",
      "      print e\n",
      "      print country[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While the *write_to_db* method will rapidly insert a dataset into the database, the tables it creates only show us one measure at a time.  When we think about the flu, we would like to look at measures for different strains side-by-side.  More importantly, when we expose that data to external tools, we'd like to present a single dataset.  Fortunately, database *views* make it easy to \"publish\" a particular query for others to quickly access.\n",
      "\n",
      "The view we want needs to do the following: for each instance of (country, year, week), produce all the measurements from the tables we created from the raw data.  That means our SQL will need to:\n",
      "\n",
      "* SELECT region, country, year, and week from **one** table\n",
      "* SELECT the measurement from **each** table with a new column name (say, the table name)\n",
      "* JOIN the tables together so that there is one row with all measures for each (region, country, year, week) tuple.\n",
      "\n",
      "Can you write the SQL query yourself?  Can you see how the *build_view* method assembles the query automatically?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_view(datasets):\n",
      "  view = \"\"\"CREATE OR REPLACE VIEW flu_statistics AS\n",
      "  SELECT a.region, a.country, a.year, a.week, \\n\"\"\"\n",
      "  from_clause = \" from \\n\"\n",
      "  where_clause = \" where \\n\"\n",
      "  column_creation = \"\"\n",
      "  table_creation = \"\"\n",
      "  join_creation = \"\"\n",
      "  for i in range(len(datasets)):\n",
      "    column_creation += \"{0}.measure as {1}\".format(string.ascii_lowercase[i], datasets[i]['title'])\n",
      "    \n",
      "    table_creation += \"{0} {1}\".format(datasets[i]['title'], string.ascii_lowercase[i])\n",
      "    \n",
      "    if (i < len(datasets)-1):\n",
      "      column_creation += \",\\n\"\n",
      "      table_creation += \",\\n\"\n",
      "      join_creation += \"{1}.country = {0}.country and {1}.year = {0}.year and {1}.week = {0}.week \\n\".format(string.ascii_lowercase[i], string.ascii_lowercase[i+1])\n",
      "      if (i < len(datasets)-2):\n",
      "        join_creation += \"and\\n\"\n",
      "    else:\n",
      "      column_creation += \"\\n\"\n",
      "      table_creation += \"\\n\"\n",
      "     \n",
      "\n",
      "  view += column_creation + from_clause + table_creation + where_clause + join_creation\n",
      "  return view"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With our methods complete, we can finally list the files and get to processing.  The glob function allows us a wildcard search of the flu statistics data we downloaded."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "files = glob(\"../flu_statistics/data/*.psv\")\n",
      "print files\n",
      "files = files[:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['/home/oracle/odsb2014/flu_statistics/data/WHO+WEST+ASIA.psv', '/home/oracle/odsb2014/flu_statistics/data/WHO+EURO.psv', '/home/oracle/odsb2014/flu_statistics/data/WHO+PAHO.psv', '/home/oracle/odsb2014/flu_statistics/data/WHO+AFRO.psv', '/home/oracle/odsb2014/flu_statistics/data/WHO+US+comprehensive.psv']\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And a simple for-loop will load the data into our database and create a view to share with our teammembers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db = cx_Oracle.connect('fludb', 'flushot', 'localhost:1521/orcl')\n",
      "print \"parsing datasets...\"\n",
      "for filename in files:\n",
      "    datasets = parseWHOCountryFile(filename)\n",
      "    print \"writing to DB...\"\n",
      "    for dataset in datasets:\n",
      "        write_to_db(db, dataset)\n",
      "    print \"creating view...\"\n",
      "    c = db.cursor()\n",
      "    c.execute(build_view(datasets))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}